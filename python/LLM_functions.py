from ollama import chat
from ollama import ChatResponse

def ChatResponse(prompt, number):
    print(f"=== ЭТАП 5.2: LLM_functions.ChatResponse() - Начало ===")
    print(f"ЭТАП 5.2.1: Параметры - prompt='{prompt}', number={number}")
    
    prompt_text = ("Придумай " + str(number) + " вопросов по теме '" + prompt + "'. "
                    "Для каждого вопроса создай 4 варианта ответа (1 правильный, 3 неправильных). "
                    "Выведи результат строго в следующем формате, БЕЗ звездочек вокруг текста:\n\n"
                    "*Вопрос*\n"
                    "Текст вопроса здесь\n"
                    "1) Первый вариант ответа\n"
                    "2) Второй вариант ответа\n"
                    "3) Третий вариант ответа\n"
                    "4) Четвертый вариант ответа\n"
                    "*правильный ответ*\n"
                    "Номер правильного варианта (1, 2, 3 или 4)\n"
                    "*объяснение*\n"
                    "Объяснение правильного ответа\n"
                    "*Следующий вопрос*\n\n"
                    "Повтори этот формат для каждого вопроса. НЕ используй звездочки вокруг текста вопросов и ответов, только для разделителей *Вопрос*, *правильный ответ*, *объяснение*, *Следующий вопрос*.")
    
    print("ЭТАП 5.2.2: Вызов chat() с моделью qwen3:8b")
    response: ChatResponse = chat(model='qwen3:8b', messages=[
      {
        'role': 'user',
        'content': prompt_text,
      },
    ])
    
    result = response['message']['content']
    print(f"ЭТАП 5.2.2 ЗАВЕРШЕН: Получен ответ от LLM, длина: {len(result)} символов")
    print(f"ЭТАП 5.2.2: Первые 200 символов ответа: {result[:200]}")
    print(f"=== ЭТАП 5.2 ЗАВЕРШЕН: LLM_functions.ChatResponse() ===")
    return result
